{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sumaiyakawsar/NLP_Masters_Class/blob/main/Lab6_NLP_070923_Language_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NmJ45sd4DfJ"
      },
      "source": [
        "### Updating and checking the NLTK version"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdqpY99J4DfV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e37c2b96-8da4-45d0-af39-71668484954f"
      },
      "source": [
        "!pip install -U pip\n",
        "!pip install -U dill\n",
        "# !pip install -U nltk==3.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-23.2.1\n",
            "Collecting dill\n",
            "  Obtaining dependency information for dill from https://files.pythonhosted.org/packages/f5/3a/74a29b11cf2cdfcd6ba89c0cecd70b37cd1ba7b77978ce611eb7a146a832/dill-0.3.7-py3-none-any.whl.metadata\n",
            "  Downloading dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\n",
            "Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dill\n",
            "Successfully installed dill-0.3.7\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb4LRyLT4DfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "163a5f21-42fc-47ec-fdc9-62dcedd3508b"
      },
      "source": [
        "import nltk\n",
        "nltk.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3.8.1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNHLNx14Dfa"
      },
      "source": [
        "# N-gram using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFGS4Zqj4Dfc"
      },
      "source": [
        "Traditionally, we can use n-grams to generate language models to predict which word comes next given a history of words.\n",
        "\n",
        "We'll use the lm module in nltk to get a sense of how non-neural language modelling is done."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJPpMnL4Dfe"
      },
      "source": [
        "from nltk.util import bigrams\n",
        "from nltk.util import ngrams\n",
        "from nltk.util import everygrams\n",
        "from nltk.util import pad_sequence\n",
        "from nltk.lm.preprocessing import pad_both_ends, flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx6Ugq5H4Dfg"
      },
      "source": [
        "If we want to train a bigram model, we need to turn this text into bigrams. Here's what the first sentence of our text would look like if we use the ngrams function from NLTK for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIxp6PEc4Dfh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d164a035-2c84-49cd-9795-9e87c18b1701"
      },
      "source": [
        "nltk.download('punkt')\n",
        "text = \"I am learning Text Analytics\"\n",
        "tokens = nltk.tokenize.word_tokenize(text.lower())\n",
        "tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'learning', 'text', 'analytics']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jvjrMxgJmGq",
        "outputId": "01004f79-bfc9-42b3-c209-ffca77b33656"
      },
      "source": [
        "list(bigrams(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'am'), ('am', 'learning'), ('learning', 'text'), ('text', 'analytics')]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnPitK4x4Dfj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df67ac8d-9149-4039-cee1-bdce10b94cb1"
      },
      "source": [
        "list(ngrams(tokens, n=3)) # n = no of grams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'am', 'learning'),\n",
              " ('am', 'learning', 'text'),\n",
              " ('learning', 'text', 'analytics')]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWdRUdqDKjJ9",
        "outputId": "009b43a8-5178-4ece-9f89-dcf878ff8413"
      },
      "source": [
        "list(everygrams(tokens, max_len=3)) # max_len will set the no of maximum grams"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i',),\n",
              " ('i', 'am'),\n",
              " ('i', 'am', 'learning'),\n",
              " ('am',),\n",
              " ('am', 'learning'),\n",
              " ('am', 'learning', 'text'),\n",
              " ('learning',),\n",
              " ('learning', 'text'),\n",
              " ('learning', 'text', 'analytics'),\n",
              " ('text',),\n",
              " ('text', 'analytics'),\n",
              " ('analytics',)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ3WIBre4Dfl"
      },
      "source": [
        "Add special \"padding\" symbols to the sentence before splitting it into ngrams. Fortunately, NLTK also has a function for that, let's see what it does to the first sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05nYk84e4Dfn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6641cee3-df03-438d-8135-ccbf732e624a"
      },
      "source": [
        "from nltk.util import pad_sequence\n",
        "list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
        "# The n order of n-grams, if it's 2-grams, you pad once, 3-grams pad twice, etc."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', 'i', 'am', 'learning', 'text', 'analytics', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JycGLGo44Dfp",
        "outputId": "8f485041-4b9e-4eb1-f6e8-132eea026c23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "padded_sent = list(pad_sequence(tokens, pad_left=True, left_pad_symbol=\"<s>\", pad_right=True, right_pad_symbol=\"</s>\", n=2))\n",
        "list(ngrams(padded_sent, n=2)) # bigram"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'i'),\n",
              " ('i', 'am'),\n",
              " ('am', 'learning'),\n",
              " ('learning', 'text'),\n",
              " ('text', 'analytics'),\n",
              " ('analytics', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFjCk4wr4Dfq"
      },
      "source": [
        "Note the n argument, that tells the function we need padding for bigrams.\n",
        "\n",
        "Now, passing all these parameters every time is tedious and in most cases they can be safely assumed as defaults anyway.\n",
        "\n",
        "Thus the nltk.lm module provides a convenience function that has all these arguments already set while the other arguments remain the same as for pad_sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW1zDLJC4Dfs",
        "outputId": "aaf83b0c-22f0-4bae-9937-d15507dc20d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk.lm.preprocessing import pad_both_ends\n",
        "list(pad_both_ends(tokens, n=3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<s>', '<s>', 'i', 'am', 'learning', 'text', 'analytics', '</s>', '</s>']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHeRcmqT4Dfv"
      },
      "source": [
        "Combining the two parts discussed so far we get the following preparation steps for one sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoSVN5Pr4Dfw",
        "outputId": "46b7a98c-41a3-4d21-f15c-4e6b72a6572e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(bigrams(pad_both_ends(tokens, n=2)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>', 'i'),\n",
              " ('i', 'am'),\n",
              " ('am', 'learning'),\n",
              " ('learning', 'text'),\n",
              " ('text', 'analytics'),\n",
              " ('analytics', '</s>')]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaaxtRWA4Dfx"
      },
      "source": [
        "To make our model more robust we could also train it on unigrams (single words) as well as bigrams, its main source of information. NLTK once again helpfully provides a function called everygrams.\n",
        "\n",
        "While not the most efficient, it is conceptually simple."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Td3s76lE4Dfz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af7a2c3a-31f2-4399-dcf1-b043fb8630a8"
      },
      "source": [
        "from nltk.util import everygrams\n",
        "padded_bigrams = list(pad_both_ends(tokens, n=2))\n",
        "list(everygrams(padded_bigrams, max_len=1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>',),\n",
              " ('i',),\n",
              " ('am',),\n",
              " ('learning',),\n",
              " ('text',),\n",
              " ('analytics',),\n",
              " ('</s>',)]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v55Up9lQ4Df0",
        "outputId": "3f54c550-e7e0-43f3-b2ca-3cdfbead0141",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(everygrams(padded_bigrams, max_len=2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('<s>',),\n",
              " ('<s>', 'i'),\n",
              " ('i',),\n",
              " ('i', 'am'),\n",
              " ('am',),\n",
              " ('am', 'learning'),\n",
              " ('learning',),\n",
              " ('learning', 'text'),\n",
              " ('text',),\n",
              " ('text', 'analytics'),\n",
              " ('analytics',),\n",
              " ('analytics', '</s>'),\n",
              " ('</s>',)]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIjJOL0_4Df0"
      },
      "source": [
        "During training and evaluation our model will rely on a vocabulary that defines which words are \"known\" to the model.\n",
        "\n",
        "To create this vocabulary we need to pad our sentences (just like for counting ngrams) and then combine the sentences into one flat stream of words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUIJsAmK4Df1"
      },
      "source": [
        "### Calculating probability of n-grams in a text / sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ispXT0vT4Df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25a99d6e-24a4-4095-9037-aa521679f8c5"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text = \"I am learning Text Analytics. I am Good\"\n",
        "# Tokenize the text.\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(text)))]\n",
        "print(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['i', 'am', 'learning', 'text', 'analytics', '.', 'i', 'am', 'good']]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZUKOrmN4Df2"
      },
      "source": [
        "# Preprocess the tokenized text for 3-grams language modelling\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE # Maximum Likelihood Estimation\n",
        "\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "model = MLE(n) # Lets train a 3-grams maximum likelihood estimation model.\n",
        "model.fit(train_data, padded_sents) # model building"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9WJHSaXLZVLF",
        "outputId": "516f0f79-02dd-4760-c755-c84c6ed887dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.lm.models.MLE at 0x7af0d3186ce0>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnYdiei54Df3"
      },
      "source": [
        "To get the counts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz1mkPaV4Df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ef9d92f-f80f-4c42-df4f-4793a2be576c"
      },
      "source": [
        "model.counts['i'] # i.e. Count('i')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfarQJmg4Df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "460c670b-7b27-47cc-bb82-094d40e7e799"
      },
      "source": [
        "model.counts[['i']]['am'] # i.e. Count('am'|'i')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-SbSr14Df5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d298a1-f516-47f6-9cbe-f4aecdbc8af3"
      },
      "source": [
        "model.counts[['i', 'am']]['learning'] # i.e. Count('learning'|'i am')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the Probablity Values:"
      ],
      "metadata": {
        "id": "AhoVGb9JM6_2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldmdNkRT4Df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dbae967-1fb1-42e2-e45d-ad2b3d1afbbd"
      },
      "source": [
        "model.score('am', 'i'.split())  # P('am'|'i') = C(i am)/C(i) = 1/1 = 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpMri5EG4Df6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e6bfcbf-c454-43f0-cd01-5120f5c035f2"
      },
      "source": [
        "model.score('learning', 'i am'.split())  # P('learning'|'i am')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHSROvTgc6QR",
        "outputId": "a91d89e8-5f2e-44f8-96af-5acd0e5ed643"
      },
      "source": [
        "len(model.vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfdsJfj8br9g",
        "outputId": "c96ef903-a082-4327-b893-5cd759aa3546"
      },
      "source": [
        "model.score(\"i\") # p(i) = c(i)/c(w)\n",
        "# tokens = 5 & pads = 4 ==> total = 9\n",
        "# c(i) = 1 & c(w) = 9"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15384615384615385"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNVz-kmRQzZb",
        "outputId": "cda5a9f2-6a33-442d-89da-3e5eb307ac1a"
      },
      "source": [
        "model.vocab.lookup(tokenized_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('i', 'am', 'learning', 'text', 'analytics', '.', 'i', 'am', 'good'),)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPAVz39gQ-is",
        "outputId": "9f6f9271-08f9-4e80-d10f-ac1fab50e266"
      },
      "source": [
        "model.vocab.lookup([\"i am playing\".split()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(('i', 'am', '<UNK>'),)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.counts[['i', 'am']]['playing'] # i.e. Count('playing'|'i am')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "901Rt-fpmGe1",
        "outputId": "2edcdd49-dffc-49c7-ec78-972977b2294d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Laplace Smoothing using NLTK**"
      ],
      "metadata": {
        "id": "M-Kzed3MRb45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import Laplace\n",
        "\n",
        "n = 3\n",
        "train_data, padded_sents = padded_everygram_pipeline(n, tokenized_text)\n",
        "\n",
        "model_laplace = Laplace(n) # Lets train a 3-grams maximum likelihood estimation model.\n",
        "model_laplace.fit(train_data, padded_sents)"
      ],
      "metadata": {
        "id": "iLcXV-3bRfY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_laplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgjOsJIMSUTk",
        "outputId": "3357d44f-7fe9-40d0-a82a-d8642f572796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<nltk.lm.models.Laplace at 0x7af0d3187130>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_laplace.counts[['i']]['am']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NByJegp_STTF",
        "outputId": "467ac6da-5662-48ba-ec5f-c81f6784f6bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_laplace.score('am', 'i'.split())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fNjiIrLSXbi",
        "outputId": "a3888344-50cc-49e6-9ad8-31e99bb38fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PM7Sdymv4Df7"
      },
      "source": [
        "## N-gram using NLTK"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jd3lMAk4Df8",
        "outputId": "5c3639f8-d386-4b0f-bde1-b9f4d40224bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "\n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "text = 'A class is a blueprint for the object.'\n",
        "\n",
        "print(\"1-gram: \", extract_ngrams(text, 1))\n",
        "print(\"2-gram: \", extract_ngrams(text, 2))\n",
        "print(\"3-gram: \", extract_ngrams(text, 3))\n",
        "print(\"4-gram: \", extract_ngrams(text, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
            "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
            "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
            "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZzeJq704Df-"
      },
      "source": [
        "## N-gram using TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v660htyc4Df-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "740caad9-5386-4a42-d3aa-498e9cfec6a2"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Function to generate n-grams from sentences.\n",
        "def extract_ngrams(data, num):\n",
        "    n_grams = TextBlob(data).ngrams(num)\n",
        "    return [ ' '.join(grams) for grams in n_grams]\n",
        "\n",
        "text = 'A class is a blueprint for the object.'\n",
        "\n",
        "print(\"1-gram: \", extract_ngrams(text, 1))\n",
        "print(\"2-gram: \", extract_ngrams(text, 2))\n",
        "print(\"3-gram: \", extract_ngrams(text, 3))\n",
        "print(\"4-gram: \", extract_ngrams(text, 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object']\n",
            "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object']\n",
            "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object']\n",
            "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "e4EonqvKvXFf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOdF3spq4Df_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09204bf-4319-4e08-e592-15c132821577"
      },
      "source": [
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE,Vocabulary\n",
        "\n",
        "train_sentences = ['an apple', 'an orange']\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in train_sentences]\n",
        "\n",
        "n = 2\n",
        "train_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
        "words = [word for sent in tokenized_text for word in sent]\n",
        "words.extend([\"<s>\", \"</s>\"])\n",
        "padded_vocab = Vocabulary(words)\n",
        "model = MLE(n)\n",
        "model.fit(train_data, padded_vocab)\n",
        "\n",
        "test_sentences = ['an apple', 'an ant']\n",
        "tokenized_text = [list(map(str.lower, nltk.tokenize.word_tokenize(sent))) for sent in test_sentences]\n",
        "\n",
        "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
        "for test in test_data:\n",
        "    print (\"MLE Estimates:\", [((ngram[-1], ngram[:-1]),model.score(ngram[-1], ngram[:-1])) for ngram in test])\n",
        "\n",
        "test_data = [nltk.bigrams(t,  pad_right=True, pad_left=True, left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\") for t in tokenized_text]\n",
        "for i, test in enumerate(test_data):\n",
        "  print(\"PP({0}):{1}\".format(test_sentences[i], model.perplexity(test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLE Estimates: [(('an', ('<s>',)), 1.0), (('apple', ('an',)), 0.5), (('</s>', ('apple',)), 1.0)]\n",
            "MLE Estimates: [(('an', ('<s>',)), 1.0), (('ant', ('an',)), 0.0), (('</s>', ('ant',)), 0)]\n",
            "PP(an apple):1.2599210498948732\n",
            "PP(an ant):inf\n"
          ]
        }
      ]
    }
  ]
}